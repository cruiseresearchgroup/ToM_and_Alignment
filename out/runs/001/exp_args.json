{
  "target_model_name": "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
  "load_model_checkpoint": "",
  "train_stimulus_completion": "",
  "train_stimulus": "",
  "train_control": "",
  "train_qa": "data/CaSiNo/train.json",
  "train_with_verb_mask": "user",
  "nudge_persona": false,
  "modify_chat_template": true,
  "filter": "",
  "train_percent": 1.0,
  "eval_ppl": true,
  "eval_stimulus_completion": "",
  "eval_stimulus": "",
  "eval_control": "",
  "eval_qa": "./data/CaSiNo/test.json",
  "eval_every_n_steps": 100,
  "output_dir": "out/runs",
  "save_model": true,
  "save_after_epoch": true,
  "use_wandb": true,
  "run_name": "test wandb",
  "shift_position_ids": true,
  "min_layer_to_read": 15,
  "max_layer_to_read": 16,
  "layer_to_write": 0,
  "module_setup": "read-vary_write-fixed_n-fixed",
  "num_layers_to_read": 1,
  "num_layers_to_sample": 1,
  "batch_size_training": 1,
  "gradient_accumulation_steps": 8,
  "gradient_clipping": false,
  "gradient_clipping_threshold": 1.0,
  "num_epochs": 10,
  "num_workers_dataloader": 1,
  "lr": 0.0001,
  "ema_decay": 1,
  "warmup_steps": 0,
  "weight_decay": 0.01,
  "gamma": 0.85,
  "seed": 42,
  "peft_method": "lora",
  "use_peft": true,
  "use_fsdp": false,
  "checkpoint_dir": "out/runs/001/checkpoints"
}