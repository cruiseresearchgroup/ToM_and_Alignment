sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.19it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  7.95it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.27it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.63it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.35it/s]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  8.17it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.42it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.47it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.81it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.65it/s]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  8.14it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.48it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.60it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.87it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.71it/s]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.25it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.05it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.65it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.39it/s]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.53it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  7.92it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.23it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.60it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.35it/s]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  8.24it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.44it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.55it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.81it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.67it/s]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  8.17it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.55it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.69it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.95it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.79it/s]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  8.01it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.44it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.63it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.92it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.73it/s]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.76it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.34it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.59it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.44it/s]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.75it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.19it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.32it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.68it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.48it/s]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
sed: -e expression #1, char 79: unknown option to `s'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.33it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.14it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.45it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.79it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.52it/s]
Traceback (most recent call last):
  File "/import/ravel/1/z5517269/latentqa/control.py", line 331, in <module>
    fire.Fire(main)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/control.py", line 319, in main
    decoder_model = get_model(
                    ^^^^^^^^^^
  File "/import/ravel/1/z5517269/latentqa/lit/utils/infra_utils.py", line 309, in get_model
    model = PeftModel.from_pretrained(model, load_peft_checkpoint)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 356, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/peft_model.py", line 730, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 249, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
	size mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 3072]) from checkpoint, the shape in current model is torch.Size([16, 4096]).
