{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_negotiationToM(string):\n",
    "    try:\n",
    "        agent1_intent = re.findall(r'1 is \\[(.*?)\\]', string)[0].split(',')\n",
    "    except:\n",
    "        agent1_intent = []\n",
    "    try:\n",
    "        agent2_intent = re.findall(r'2 is \\[(.*?)\\]', string)[0].split(',')\n",
    "    except:\n",
    "        agent2_intent = []\n",
    "    agent1_pattern = r'Agent 1, Desire High: (.*?), Desire Medium: (.*?), Desire Low: (.*?),  Belief High: (.*?), Belief Medium: (.*?), Belief Low: (.*?)\\.'\n",
    "    try:\n",
    "        agent1_desire_belief = re.findall(agent1_pattern, string)[0]\n",
    "    except:\n",
    "        agent1_desire_belief = ('Not Given', 'Not Given', 'Not Given', 'Firewood', 'Not Given', 'Not Given')\n",
    "    agent2_pattern = r'Agent 2, Desire High: (.*?), Desire Medium: (.*?), Desire Low: (.*?),  Belief High: (.*?), Belief Medium: (.*?), Belief Low: (.*?)\\.'\n",
    "    try:\n",
    "        agent2_desire_belief = re.findall(agent2_pattern, string)[0]\n",
    "    except:\n",
    "        agent2_desire_belief = ('Not Given', 'Not Given', 'Not Given', 'Firewood', 'Not Given', 'Not Given')\n",
    "    return agent1_intent, agent2_intent, agent1_desire_belief, agent2_desire_belief\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Score is:  0.1518987341772152\n",
      "Desire Score is:  0.38396624472573837\n",
      "Belief Score is:  0.38115330520393814\n"
     ]
    }
   ],
   "source": [
    "with open('controls/NegotiationToM Middle Llama-3 8B.jsonl', 'r') as fin:\n",
    "    NegotiationToM = json.load(fin)\n",
    "score = 0\n",
    "belief = 0\n",
    "desire = 0\n",
    "intention = 0\n",
    "for idx, item in enumerate(NegotiationToM):\n",
    "    gt_agent1_intent, gt_agent2_intent, gt_agent1_desire_belief, gt_agent2_desire_belief = extract_data_negotiationToM(item['ground_truth'])\n",
    "    pr_agent1_intent, pr_agent2_intent, pr_agent1_desire_belief, pr_agent2_desire_belief = extract_data_negotiationToM(item['response'])\n",
    "    if gt_agent1_intent==pr_agent1_intent:\n",
    "        if gt_agent2_intent==pr_agent2_intent:\n",
    "            if gt_agent1_desire_belief==pr_agent1_desire_belief:\n",
    "                if gt_agent2_desire_belief==pr_agent2_desire_belief:\n",
    "                    score+=1\n",
    "    if (gt_agent1_desire_belief[:3]==pr_agent1_desire_belief[:3]) and (gt_agent2_desire_belief[:3]==pr_agent2_desire_belief[:3]):\n",
    "        desire+=1\n",
    "        \n",
    "    if (gt_agent1_desire_belief[3:]==pr_agent1_desire_belief[3:]) and (gt_agent2_desire_belief[3:]==pr_agent2_desire_belief[3:]):\n",
    "        belief+=1\n",
    "        \n",
    "\n",
    "print(\"All Score is: \", score/len(NegotiationToM))\n",
    "print(\"Desire Score is: \", desire/len(NegotiationToM))\n",
    "print(\"Belief Score is: \", belief/len(NegotiationToM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Score is:  0.06188466947960619\n",
      "Desire Score is:  0.2390998593530239\n",
      "Belief Score is:  0.20956399437412096\n"
     ]
    }
   ],
   "source": [
    "with open('controls/NegotiationToM Middle Llama-3 3B.jsonl', 'r') as fin:\n",
    "    NegotiationToM = json.load(fin)\n",
    "score = 0\n",
    "belief = 0\n",
    "desire = 0\n",
    "intention = 0\n",
    "for idx, item in enumerate(NegotiationToM):\n",
    "    gt_agent1_intent, gt_agent2_intent, gt_agent1_desire_belief, gt_agent2_desire_belief = extract_data_negotiationToM(item['ground_truth'])\n",
    "    pr_agent1_intent, pr_agent2_intent, pr_agent1_desire_belief, pr_agent2_desire_belief = extract_data_negotiationToM(item['response'])\n",
    "    if gt_agent1_intent==pr_agent1_intent:\n",
    "        if gt_agent2_intent==pr_agent2_intent:\n",
    "            if gt_agent1_desire_belief==pr_agent1_desire_belief:\n",
    "                if gt_agent2_desire_belief==pr_agent2_desire_belief:\n",
    "                    score+=1\n",
    "    if (gt_agent1_desire_belief[:3]==pr_agent1_desire_belief[:3]) and (gt_agent2_desire_belief[:3]==pr_agent2_desire_belief[:3]):\n",
    "        desire+=1\n",
    "        \n",
    "    if (gt_agent1_desire_belief[3:]==pr_agent1_desire_belief[3:]) and (gt_agent2_desire_belief[3:]==pr_agent2_desire_belief[3:]):\n",
    "        belief+=1\n",
    "        \n",
    "\n",
    "print(\"All Score is: \", score/len(NegotiationToM))\n",
    "print(\"Desire Score is: \", desire/len(NegotiationToM))\n",
    "print(\"Belief Score is: \", belief/len(NegotiationToM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Score is:  0.03938115330520394\n",
      "Desire Score is:  0.24331926863572434\n",
      "Belief Score is:  0.14345991561181434\n"
     ]
    }
   ],
   "source": [
    "with open('controls/NegotiationToM CoT Llama-3 3B.jsonl', 'r') as fin:\n",
    "    NegotiationToM = json.load(fin)\n",
    "score = 0\n",
    "belief = 0\n",
    "desire = 0\n",
    "intention = 0\n",
    "for idx, item in enumerate(NegotiationToM):\n",
    "    gt_agent1_intent, gt_agent2_intent, gt_agent1_desire_belief, gt_agent2_desire_belief = extract_data_negotiationToM(item['ground_truth'])\n",
    "    pr_agent1_intent, pr_agent2_intent, pr_agent1_desire_belief, pr_agent2_desire_belief = extract_data_negotiationToM(item['response'])\n",
    "    if gt_agent1_intent==pr_agent1_intent:\n",
    "        if gt_agent2_intent==pr_agent2_intent:\n",
    "            if gt_agent1_desire_belief==pr_agent1_desire_belief:\n",
    "                if gt_agent2_desire_belief==pr_agent2_desire_belief:\n",
    "                    score+=1\n",
    "    if (gt_agent1_desire_belief[:3]==pr_agent1_desire_belief[:3]) and (gt_agent2_desire_belief[:3]==pr_agent2_desire_belief[:3]):\n",
    "        desire+=1\n",
    "        \n",
    "    if (gt_agent1_desire_belief[3:]==pr_agent1_desire_belief[3:]) and (gt_agent2_desire_belief[3:]==pr_agent2_desire_belief[3:]):\n",
    "        belief+=1\n",
    "        \n",
    "\n",
    "print(\"All Score is: \", score/len(NegotiationToM))\n",
    "print(\"Desire Score is: \", desire/len(NegotiationToM))\n",
    "print(\"Belief Score is: \", belief/len(NegotiationToM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Score is:  0.05485232067510549\n",
      "Desire Score is:  0.35864978902953587\n",
      "Belief Score is:  0.17580872011251758\n"
     ]
    }
   ],
   "source": [
    "with open('controls/NegotiationToM CoT Llama-3 8B.jsonl', 'r') as fin:\n",
    "    NegotiationToM = json.load(fin)\n",
    "score = 0\n",
    "belief = 0\n",
    "desire = 0\n",
    "intention = 0\n",
    "for idx, item in enumerate(NegotiationToM):\n",
    "    gt_agent1_intent, gt_agent2_intent, gt_agent1_desire_belief, gt_agent2_desire_belief = extract_data_negotiationToM(item['ground_truth'])\n",
    "    pr_agent1_intent, pr_agent2_intent, pr_agent1_desire_belief, pr_agent2_desire_belief = extract_data_negotiationToM(item['response'])\n",
    "    if gt_agent1_intent==pr_agent1_intent:\n",
    "        if gt_agent2_intent==pr_agent2_intent:\n",
    "            if gt_agent1_desire_belief==pr_agent1_desire_belief:\n",
    "                if gt_agent2_desire_belief==pr_agent2_desire_belief:\n",
    "                    score+=1\n",
    "    if (gt_agent1_desire_belief[:3]==pr_agent1_desire_belief[:3]) and (gt_agent2_desire_belief[:3]==pr_agent2_desire_belief[:3]):\n",
    "        desire+=1\n",
    "        \n",
    "    if (gt_agent1_desire_belief[3:]==pr_agent1_desire_belief[3:]) and (gt_agent2_desire_belief[3:]==pr_agent2_desire_belief[3:]):\n",
    "        belief+=1\n",
    "        \n",
    "\n",
    "print(\"All Score is: \", score/len(NegotiationToM))\n",
    "print(\"Desire Score is: \", desire/len(NegotiationToM))\n",
    "print(\"Belief Score is: \", belief/len(NegotiationToM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AIML_API_KEY'] = 'cd610f9c790e4905a55a4f97182e72eb'\n",
    "os.environ['NVIDIA_API_KEY'] = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/import/ravel/1/z5517269/latentqa/baselines/fantom\n"
     ]
    }
   ],
   "source": [
    "%cd ./baselines/fantom/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already built at data/fantom. version 1.0\n",
      "Generating responses...\n",
      "File /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results/model_responses_full_input_Meta-Llama-3.1-8B-Instruct-Turbo_cot-True.jsonl exists. Reading responses from file...\n",
      "100%|███████████████████████████████████| 3795/3795 [00:00<00:00, 124521.89it/s]\n",
      "Running evaluation...\n",
      "100%|██████████████████████████████████████| 3795/3795 [00:11<00:00, 343.36it/s]\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "\n",
      "[[ FANToM input type: full ]]\n",
      "[[ Model: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo ]]\n",
      "\n",
      "model : meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
      "\n",
      "conversation_input_type : full\n",
      "\n",
      "inaccessible:set:ALL* : 0.0\n",
      "\n",
      "inaccessible:set:ALL : 0.0\n",
      "\n",
      "inaccessible:belief:multiple-choice : 0.0\n",
      "\n",
      "inaccessible:belief:distance : 41.3\n",
      "\n",
      "inaccessible:belief_true_word-f1 : 30.099999999999998\n",
      "\n",
      "inaccessible:answerability:set:ALL : 2.7\n",
      "\n",
      "inaccessible:answerability:list : 31.7\n",
      "\n",
      "inaccessible:answerability:binary-f1 : 55.2\n",
      "\n",
      "inaccessible:info_accessibility:set:ALL : 6.0\n",
      "\n",
      "inaccessible:info_accessibility:list : 27.500000000000004\n",
      "\n",
      "inaccessible:info_accessibility:binary-f1 : 66.3\n",
      "\n",
      "fact_word-f1 : 38.9\n",
      "\n",
      "inaccessible:tom:lists:wrong_reasons:freq : {'included_unaware_character': 81, 'excluded_aware_character': 51, 'did_both': 17}\n",
      "\n",
      "inaccessible:tom:binary:wrong_reasons:freq : {'false_negative': 529, 'false_positive': 161, 'irrelevant_response': 40}\n",
      "\n",
      "inaccessible:first-order : 33.900000000000006\n",
      "\n",
      "inaccessible:second-order : 55.7\n",
      "\n",
      "inaccessible:second-order:acyclic : 60.4\n",
      "\n",
      "inaccessible:second-order:cyclic : 51.0\n",
      "\n",
      "inaccessible:set:ALL_character : 19.6\n",
      "\n",
      "inaccessible:set:character_answer_consistency : 26.400000000000002\n",
      "\n",
      ">>>>> Dumped evaluation outputs and the report at /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results!\n",
      ">>>>> Evaluated model responses filename: evaluated_responses_full_input_Meta-Llama-3.1-8B-Instruct-Turbo_cot-True.json\n",
      ">>>>> REPORT filename: REPORT_full_input_Meta-Llama-3.1-8B-Instruct-Turbo_cot-True.json\n"
     ]
    }
   ],
   "source": [
    "!python ./eval_fantom.py --model 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo' --conversation-input-type 'full' --use-cot True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already built at data/fantom. version 1.0\n",
      "Generating responses...\n",
      "File /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results/model_responses_full_input_Llama-3.2-3B-Instruct-Turbo_cot-True.jsonl exists. Reading responses from file...\n",
      "100%|███████████████████████████████████| 3795/3795 [00:00<00:00, 108256.48it/s]\n",
      "Running evaluation...\n",
      "100%|██████████████████████████████████████| 3795/3795 [00:11<00:00, 335.34it/s]\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "\n",
      "[[ FANToM input type: full ]]\n",
      "[[ Model: meta-llama/Llama-3.2-3B-Instruct-Turbo ]]\n",
      "\n",
      "model : meta-llama/Llama-3.2-3B-Instruct-Turbo\n",
      "\n",
      "conversation_input_type : full\n",
      "\n",
      "inaccessible:set:ALL* : 0.0\n",
      "\n",
      "inaccessible:set:ALL : 0.0\n",
      "\n",
      "inaccessible:belief:multiple-choice : 0.0\n",
      "\n",
      "inaccessible:belief:distance : 41.3\n",
      "\n",
      "inaccessible:belief_true_word-f1 : 30.099999999999998\n",
      "\n",
      "inaccessible:answerability:set:ALL : 2.7\n",
      "\n",
      "inaccessible:answerability:list : 31.7\n",
      "\n",
      "inaccessible:answerability:binary-f1 : 55.2\n",
      "\n",
      "inaccessible:info_accessibility:set:ALL : 6.0\n",
      "\n",
      "inaccessible:info_accessibility:list : 27.500000000000004\n",
      "\n",
      "inaccessible:info_accessibility:binary-f1 : 66.3\n",
      "\n",
      "fact_word-f1 : 38.9\n",
      "\n",
      "inaccessible:tom:lists:wrong_reasons:freq : {'included_unaware_character': 81, 'excluded_aware_character': 51, 'did_both': 17}\n",
      "\n",
      "inaccessible:tom:binary:wrong_reasons:freq : {'false_negative': 529, 'false_positive': 161, 'irrelevant_response': 40}\n",
      "\n",
      "inaccessible:first-order : 33.900000000000006\n",
      "\n",
      "inaccessible:second-order : 55.7\n",
      "\n",
      "inaccessible:second-order:acyclic : 60.4\n",
      "\n",
      "inaccessible:second-order:cyclic : 51.0\n",
      "\n",
      "inaccessible:set:ALL_character : 19.6\n",
      "\n",
      "inaccessible:set:character_answer_consistency : 26.400000000000002\n",
      "\n",
      ">>>>> Dumped evaluation outputs and the report at /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results!\n",
      ">>>>> Evaluated model responses filename: evaluated_responses_full_input_Llama-3.2-3B-Instruct-Turbo_cot-True.json\n",
      ">>>>> REPORT filename: REPORT_full_input_Llama-3.2-3B-Instruct-Turbo_cot-True.json\n"
     ]
    }
   ],
   "source": [
    "!python eval_fantom.py --model 'meta-llama/Llama-3.2-3B-Instruct-Turbo' --conversation-input-type 'full' --use-cot True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already built at data/fantom. version 1.0\n",
      "Generating responses...\n",
      "File /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results/model_responses_full_input_FANTOM Middle Llama-3 3B_cot-True.jsonl exists. Reading responses from file...\n",
      "100%|███████████████████████████████████| 3795/3795 [00:00<00:00, 124690.64it/s]\n",
      "Running evaluation...\n",
      "100%|██████████████████████████████████████| 3795/3795 [00:11<00:00, 336.79it/s]\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "\n",
      "[[ FANToM input type: full ]]\n",
      "[[ Model: FANTOM Middle Llama-3 3B ]]\n",
      "\n",
      "model : FANTOM Middle Llama-3 3B\n",
      "\n",
      "conversation_input_type : full\n",
      "\n",
      "inaccessible:set:ALL* : 11.899999999999999\n",
      "\n",
      "inaccessible:set:ALL : 25.1\n",
      "\n",
      "inaccessible:belief:multiple-choice : 51.7\n",
      "\n",
      "inaccessible:belief:distance : 46.5\n",
      "\n",
      "inaccessible:belief_true_word-f1 : 72.2\n",
      "\n",
      "inaccessible:answerability:set:ALL : 64.4\n",
      "\n",
      "inaccessible:answerability:list : 76.6\n",
      "\n",
      "inaccessible:answerability:binary-f1 : 92.60000000000001\n",
      "\n",
      "inaccessible:info_accessibility:set:ALL : 63.800000000000004\n",
      "\n",
      "inaccessible:info_accessibility:list : 75.2\n",
      "\n",
      "inaccessible:info_accessibility:binary-f1 : 93.0\n",
      "\n",
      "fact_word-f1 : 44.3\n",
      "\n",
      "inaccessible:tom:lists:wrong_reasons:freq : {'included_unaware_character': 30, 'excluded_aware_character': 17, 'did_both': 4}\n",
      "\n",
      "inaccessible:tom:binary:wrong_reasons:freq : {'false_negative': 39, 'false_positive': 89, 'irrelevant_response': 5}\n",
      "\n",
      "inaccessible:first-order : 37.6\n",
      "\n",
      "inaccessible:second-order : 63.9\n",
      "\n",
      "inaccessible:second-order:acyclic : 66.7\n",
      "\n",
      "inaccessible:second-order:cyclic : 61.199999999999996\n",
      "\n",
      "inaccessible:set:ALL_character : 69.1\n",
      "\n",
      "inaccessible:set:character_answer_consistency : 90.9\n",
      "\n",
      ">>>>> Dumped evaluation outputs and the report at /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results!\n",
      ">>>>> Evaluated model responses filename: evaluated_responses_full_input_FANTOM Middle Llama-3 3B_cot-True.json\n",
      ">>>>> REPORT filename: REPORT_full_input_FANTOM Middle Llama-3 3B_cot-True.json\n"
     ]
    }
   ],
   "source": [
    "!python eval_fantom.py --model 'FANTOM Middle Llama-3 3B' --conversation-input-type 'full' --use-cot True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already built at data/fantom. version 1.0\n",
      "Generating responses...\n",
      "File /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results/model_responses_full_input_FANTOM Middle Llama-3 8B_cot-True.jsonl exists. Reading responses from file...\n",
      "100%|███████████████████████████████████| 3795/3795 [00:00<00:00, 125579.95it/s]\n",
      "Running evaluation...\n",
      "100%|██████████████████████████████████████| 3795/3795 [00:11<00:00, 339.84it/s]\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "\n",
      "[[ FANToM input type: full ]]\n",
      "[[ Model: FANTOM Middle Llama-3 8B ]]\n",
      "\n",
      "model : FANTOM Middle Llama-3 8B\n",
      "\n",
      "conversation_input_type : full\n",
      "\n",
      "inaccessible:set:ALL* : 16.400000000000002\n",
      "\n",
      "inaccessible:set:ALL : 22.8\n",
      "\n",
      "inaccessible:belief:multiple-choice : 49.7\n",
      "\n",
      "inaccessible:belief:distance : 66.10000000000001\n",
      "\n",
      "inaccessible:belief_true_word-f1 : 79.0\n",
      "\n",
      "inaccessible:answerability:set:ALL : 67.60000000000001\n",
      "\n",
      "inaccessible:answerability:list : 73.4\n",
      "\n",
      "inaccessible:answerability:binary-f1 : 94.19999999999999\n",
      "\n",
      "inaccessible:info_accessibility:set:ALL : 61.5\n",
      "\n",
      "inaccessible:info_accessibility:list : 71.6\n",
      "\n",
      "inaccessible:info_accessibility:binary-f1 : 92.80000000000001\n",
      "\n",
      "fact_word-f1 : 51.1\n",
      "\n",
      "inaccessible:tom:lists:wrong_reasons:freq : {'included_unaware_character': 37, 'excluded_aware_character': 20, 'did_both': 1}\n",
      "\n",
      "inaccessible:tom:binary:wrong_reasons:freq : {'false_negative': 22, 'false_positive': 94}\n",
      "\n",
      "inaccessible:first-order : 64.0\n",
      "\n",
      "inaccessible:second-order : 70.1\n",
      "\n",
      "inaccessible:second-order:acyclic : 75.0\n",
      "\n",
      "inaccessible:second-order:cyclic : 65.3\n",
      "\n",
      "inaccessible:set:ALL_character : 73.1\n",
      "\n",
      "inaccessible:set:character_answer_consistency : 91.60000000000001\n",
      "\n",
      ">>>>> Dumped evaluation outputs and the report at /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results!\n",
      ">>>>> Evaluated model responses filename: evaluated_responses_full_input_FANTOM Middle Llama-3 8B_cot-True.json\n",
      ">>>>> REPORT filename: REPORT_full_input_FANTOM Middle Llama-3 8B_cot-True.json\n"
     ]
    }
   ],
   "source": [
    "!python eval_fantom.py --model 'FANTOM Middle Llama-3 8B' --conversation-input-type 'full' --use-cot True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latentqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
