{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_negotiationToM(string):\n",
    "    try:\n",
    "        agent1_intent = re.findall(r'1 is \\[(.*?)\\]', string)[0].split(',')\n",
    "    except:\n",
    "        agent1_intent = []\n",
    "    try:\n",
    "        agent2_intent = re.findall(r'2 is \\[(.*?)\\]', string)[0].split(',')\n",
    "    except:\n",
    "        agent2_intent = []\n",
    "    agent1_pattern = r'Agent 1, Desire High: (.*?), Desire Medium: (.*?), Desire Low: (.*?),  Belief High: (.*?), Belief Medium: (.*?), Belief Low: (.*?)\\.'\n",
    "    try:\n",
    "        agent1_desire_belief = re.findall(agent1_pattern, string)[0]\n",
    "    except:\n",
    "        agent1_desire_belief = ('Not Given', 'Not Given', 'Not Given', 'Firewood', 'Not Given', 'Not Given')\n",
    "    agent2_pattern = r'Agent 2, Desire High: (.*?), Desire Medium: (.*?), Desire Low: (.*?),  Belief High: (.*?), Belief Medium: (.*?), Belief Low: (.*?)\\.'\n",
    "    try:\n",
    "        agent2_desire_belief = re.findall(agent2_pattern, string)[0]\n",
    "    except:\n",
    "        agent2_desire_belief = ('Not Given', 'Not Given', 'Not Given', 'Firewood', 'Not Given', 'Not Given')\n",
    "    return agent1_intent, agent2_intent, agent1_desire_belief, agent2_desire_belief\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegotiationToM_eval(path):\n",
    "    with open(path, 'r') as fin:\n",
    "        NegotiationToM = json.load(fin)\n",
    "    score = 0\n",
    "    belief = 0\n",
    "    desire = 0\n",
    "    intention_actual, intention_prediction = [], []\n",
    "    for idx, item in enumerate(NegotiationToM):\n",
    "        gt_agent1_intent, gt_agent2_intent, gt_agent1_desire_belief, gt_agent2_desire_belief = extract_data_negotiationToM(item['ground_truth'])\n",
    "        pr_agent1_intent, pr_agent2_intent, pr_agent1_desire_belief, pr_agent2_desire_belief = extract_data_negotiationToM(item['response'])\n",
    "        intention_actual.append(gt_agent1_intent+gt_agent2_intent)\n",
    "        intention_prediction.append(pr_agent1_intent+pr_agent2_intent)\n",
    "        if gt_agent1_intent==pr_agent1_intent:\n",
    "            if gt_agent2_intent==pr_agent2_intent:\n",
    "                if gt_agent1_desire_belief==pr_agent1_desire_belief:\n",
    "                    if gt_agent2_desire_belief==pr_agent2_desire_belief:\n",
    "                        score+=1\n",
    "        if (gt_agent1_desire_belief[:3]==pr_agent1_desire_belief[:3]) and (gt_agent2_desire_belief[:3]==pr_agent2_desire_belief[:3]):\n",
    "            desire+=1\n",
    "            \n",
    "        if (gt_agent1_desire_belief[3:]==pr_agent1_desire_belief[3:]) and (gt_agent2_desire_belief[3:]==pr_agent2_desire_belief[3:]):\n",
    "            belief+=1\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_true_bin = mlb.fit_transform(intention_actual)\n",
    "    y_pred_bin = mlb.transform(intention_prediction)  \n",
    "    f1_micro = f1_score(y_true_bin, y_pred_bin, average='micro')\n",
    "    f1_macro = f1_score(y_true_bin, y_pred_bin, average='macro')\n",
    "\n",
    "    print(\"All Score : \", score/len(NegotiationToM))\n",
    "    print(\"Desire Accuracy: \", desire/len(NegotiationToM))\n",
    "    print(\"Belief Accuracy: \", belief/len(NegotiationToM))\n",
    "    print(\"Intention F1 Micro Average:\", f1_micro)\n",
    "    print(\"Intention F1 Macro Average:\", f1_macro)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Score :  0.1518987341772152\n",
      "Desire Accuracy:  0.38396624472573837\n",
      "Belief Accuracy:  0.38115330520393814\n",
      "Intention F1 Micro Average: 0.6598639455782312\n",
      "Intention F1 Macro Average: 0.5243034246212535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['Agree', 'Callout-Fair', 'Do-Not-Care', 'Fairness', 'Inquire', 'No-Preference', 'No-Understand', 'Show-Need', 'Undermine-Reference', 'Understand', 'Understand-Requirements', 'Understand-Why', 'Water'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "NegotiationToM_eval('controls/NegotiationToM Middle Llama-3 8B.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Score :  0.06188466947960619\n",
      "Desire Accuracy:  0.2390998593530239\n",
      "Belief Accuracy:  0.20956399437412096\n",
      "Intention F1 Micro Average: 0.40131578947368424\n",
      "Intention F1 Macro Average: 0.18814550993910334\n"
     ]
    }
   ],
   "source": [
    "NegotiationToM_eval('controls/NegotiationToM Middle Llama-3 3B.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Score :  0.05485232067510549\n",
      "Desire Accuracy:  0.35864978902953587\n",
      "Belief Accuracy:  0.17580872011251758\n",
      "Intention F1 Micro Average: 0.4052241440169432\n",
      "Intention F1 Macro Average: 0.2895797585985143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) [' Build-Rapport', ' Callout-Fairness', ' Describe-Need', ' Discover-Preference', ' Promote-Coordination'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "NegotiationToM_eval('controls/NegotiationToM CoT Llama-3 8B.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Score :  0.03938115330520394\n",
      "Desire Accuracy:  0.24331926863572434\n",
      "Belief Accuracy:  0.14345991561181434\n",
      "Intention F1 Micro Average: 0.2957696409527195\n",
      "Intention F1 Macro Average: 0.23282086931336404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) [' Callout-Fairness', ' Describe-Need', ' Discover-Preference', ' No-Intention', ' No-Need', ' Promote-Coordination', ' Show-Empathy', 'Callout-Reality', 'Callout-Trade', 'Show-Gratitude'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "NegotiationToM_eval('controls/NegotiationToM CoT Llama-3 3B.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Score :  0.11251758087201125\n",
      "Desire Accuracy:  0.3656821378340366\n",
      "Belief Accuracy:  0.36427566807313644\n",
      "Intention F1 Micro Average: 0.5521669341894061\n",
      "Intention F1 Macro Average: 0.32665306882650336\n"
     ]
    }
   ],
   "source": [
    "NegotiationToM_eval('controls/NegotiationToM FT Llama-3 3B.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Score :  0.17721518987341772\n",
      "Desire Accuracy:  0.5021097046413502\n",
      "Belief Accuracy:  0.4964838255977497\n",
      "Intention F1 Micro Average: 0.6424751718869366\n",
      "Intention F1 Macro Average: 0.43876737806197397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['Discover-Need'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "NegotiationToM_eval('controls/NegotiationToM FT Llama-3 8B.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Score :  0.04781997187060478\n",
      "Desire Accuracy:  0.31645569620253167\n",
      "Belief Accuracy:  0.17862165963431786\n",
      "Intention F1 Micro Average: 0.4506531204644412\n",
      "Intention F1 Macro Average: 0.35416582077836406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) [' Build-Rapport', ' Callout-Fairness', ' Describe-Need', ' Discover-Preference', ' No-Need', ' Promote-Coordination', ' Show-Empathy', ' Undermine-Requirements', 'NoIntention'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NegotiationToM_eval('controls/NegotiationToM CoT gpt-4o-mini 8B.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AIML_API_KEY'] = 'cd610f9c790e4905a55a4f97182e72eb'\n",
    "os.environ['NVIDIA_API_KEY'] = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/import/ravel/1/z5517269/latentqa/baselines/tabel2/CoT/FanToM\n"
     ]
    }
   ],
   "source": [
    "%cd baselines/tabel2/CoT/FanToM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./eval_fantom.py --model 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo' --conversation-input-type 'full' --use-cot True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already built at data/fantom. version 1.0\n",
      "Generating responses...\n",
      "File /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results/model_responses_full_input_Meta-Llama-3.1-8B-Instruct-Turbo_cot-True.jsonl exists. Reading responses from file...\n",
      "100%|███████████████████████████████████| 3795/3795 [00:00<00:00, 110986.73it/s]\n",
      "Running evaluation...\n",
      "100%|██████████████████████████████████████| 3795/3795 [00:11<00:00, 318.15it/s]\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "\n",
      "[[ FANToM input type: full ]]\n",
      "[[ Model: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo ]]\n",
      "\n",
      "model : meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
      "\n",
      "conversation_input_type : full\n",
      "\n",
      "inaccessible:set:ALL* : 0.0\n",
      "\n",
      "inaccessible:set:ALL : 0.0\n",
      "\n",
      "inaccessible:belief:multiple-choice : 0.0\n",
      "\n",
      "inaccessible:belief:distance : 41.3\n",
      "\n",
      "inaccessible:belief_true_word-f1 : 30.099999999999998\n",
      "\n",
      "inaccessible:answerability:set:ALL : 2.7\n",
      "\n",
      "inaccessible:answerability:list : 31.7\n",
      "\n",
      "inaccessible:answerability:binary-f1 : 55.2\n",
      "\n",
      "inaccessible:info_accessibility:set:ALL : 6.0\n",
      "\n",
      "inaccessible:info_accessibility:list : 27.500000000000004\n",
      "\n",
      "inaccessible:info_accessibility:binary-f1 : 66.3\n",
      "\n",
      "fact_word-f1 : 38.9\n",
      "\n",
      "inaccessible:tom:lists:wrong_reasons:freq : {'included_unaware_character': 81, 'excluded_aware_character': 51, 'did_both': 17}\n",
      "\n",
      "inaccessible:tom:binary:wrong_reasons:freq : {'false_negative': 529, 'false_positive': 161, 'irrelevant_response': 40}\n",
      "\n",
      "inaccessible:first-order : 33.900000000000006\n",
      "\n",
      "inaccessible:second-order : 55.7\n",
      "\n",
      "inaccessible:second-order:acyclic : 60.4\n",
      "\n",
      "inaccessible:second-order:cyclic : 51.0\n",
      "\n",
      "inaccessible:set:ALL_character : 19.6\n",
      "\n",
      "inaccessible:set:character_answer_consistency : 26.400000000000002\n",
      "\n",
      ">>>>> Dumped evaluation outputs and the report at /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results!\n",
      ">>>>> Evaluated model responses filename: evaluated_responses_full_input_Meta-Llama-3.1-8B-Instruct-Turbo_cot-True.json\n",
      ">>>>> REPORT filename: REPORT_full_input_Meta-Llama-3.1-8B-Instruct-Turbo_cot-True.json\n"
     ]
    }
   ],
   "source": [
    "!python ./eval_fantom.py --model 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo' --conversation-input-type 'full' --use-cot True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already built at data/fantom. version 1.0\n",
      "Generating responses...\n",
      "File /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results/model_responses_full_input_Llama-3.2-3B-Instruct-Turbo_cot-True.jsonl exists. Reading responses from file...\n",
      "100%|███████████████████████████████████| 3795/3795 [00:00<00:00, 127057.51it/s]\n",
      "Running evaluation...\n",
      "100%|██████████████████████████████████████| 3795/3795 [00:11<00:00, 341.01it/s]\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "\n",
      "[[ FANToM input type: full ]]\n",
      "[[ Model: meta-llama/Llama-3.2-3B-Instruct-Turbo ]]\n",
      "\n",
      "model : meta-llama/Llama-3.2-3B-Instruct-Turbo\n",
      "\n",
      "conversation_input_type : full\n",
      "\n",
      "inaccessible:set:ALL* : 0.0\n",
      "\n",
      "inaccessible:set:ALL : 0.0\n",
      "\n",
      "inaccessible:belief:multiple-choice : 0.0\n",
      "\n",
      "inaccessible:belief:distance : 30.099999999999998\n",
      "\n",
      "inaccessible:belief_true_word-f1 : 29.299999999999997\n",
      "\n",
      "inaccessible:answerability:set:ALL : 1.7999999999999998\n",
      "\n",
      "inaccessible:answerability:list : 30.7\n",
      "\n",
      "inaccessible:answerability:binary-f1 : 42.9\n",
      "\n",
      "inaccessible:info_accessibility:set:ALL : 2.3\n",
      "\n",
      "inaccessible:info_accessibility:list : 17.4\n",
      "\n",
      "inaccessible:info_accessibility:binary-f1 : 60.8\n",
      "\n",
      "fact_word-f1 : 37.4\n",
      "\n",
      "inaccessible:tom:lists:wrong_reasons:freq : {'excluded_aware_character': 74, 'included_unaware_character': 50, 'did_both': 27}\n",
      "\n",
      "inaccessible:tom:binary:wrong_reasons:freq : {'false_negative': 581, 'false_positive': 212, 'irrelevant_response': 106}\n",
      "\n",
      "inaccessible:first-order : 23.3\n",
      "\n",
      "inaccessible:second-order : 43.3\n",
      "\n",
      "inaccessible:second-order:acyclic : 43.8\n",
      "\n",
      "inaccessible:second-order:cyclic : 42.9\n",
      "\n",
      "inaccessible:set:ALL_character : 11.200000000000001\n",
      "\n",
      "inaccessible:set:character_answer_consistency : 16.400000000000002\n",
      "\n",
      ">>>>> Dumped evaluation outputs and the report at /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results!\n",
      ">>>>> Evaluated model responses filename: evaluated_responses_full_input_Llama-3.2-3B-Instruct-Turbo_cot-True.json\n",
      ">>>>> REPORT filename: REPORT_full_input_Llama-3.2-3B-Instruct-Turbo_cot-True.json\n"
     ]
    }
   ],
   "source": [
    "!python eval_fantom.py --model 'meta-llama/Llama-3.2-3B-Instruct-Turbo' --conversation-input-type 'full' --use-cot True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "Already built at data/fantom. version 1.0\n",
      "Generating responses...\n",
      "File /import/ravel/1/z5517269/latentqa/baselines/tabel2/CoT/FanToM/data/results/model_responses_full_input_FANTOM FT Llama-3 3B_cot-True.jsonl exists. Reading responses from file...\n",
      "100%|███████████████████████████████████| 3795/3795 [00:00<00:00, 117275.86it/s]\n",
      "Running evaluation...\n",
      "100%|███████████████████████████████████████| 3795/3795 [01:36<00:00, 39.25it/s]\n",
      "Downloading builder script: 100%|██████████| 6.79k/6.79k [00:00<00:00, 20.9MB/s]\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "\n",
      "[[ FANToM input type: full ]]\n",
      "[[ Model: FANTOM FT Llama-3 3B ]]\n",
      "\n",
      "model : FANTOM FT Llama-3 3B\n",
      "\n",
      "conversation_input_type : full\n",
      "\n",
      "inaccessible:set:ALL* : 8.200000000000001\n",
      "\n",
      "inaccessible:set:ALL : 11.0\n",
      "\n",
      "inaccessible:belief:multiple-choice : 40.6\n",
      "\n",
      "inaccessible:belief:distance : 53.1\n",
      "\n",
      "inaccessible:belief_true_word-f1 : 79.5\n",
      "\n",
      "inaccessible:answerability:set:ALL : 37.4\n",
      "\n",
      "inaccessible:answerability:list : 66.10000000000001\n",
      "\n",
      "inaccessible:answerability:binary-f1 : 87.2\n",
      "\n",
      "inaccessible:info_accessibility:set:ALL : 39.4\n",
      "\n",
      "inaccessible:info_accessibility:list : 57.3\n",
      "\n",
      "inaccessible:info_accessibility:binary-f1 : 88.9\n",
      "\n",
      "fact_word-f1 : 52.7\n",
      "\n",
      "inaccessible:tom:lists:wrong_reasons:freq : {'included_unaware_character': 42, 'excluded_aware_character': 27, 'did_both': 5}\n",
      "\n",
      "inaccessible:tom:binary:wrong_reasons:freq : {'false_negative': 57, 'false_positive': 157}\n",
      "\n",
      "inaccessible:first-order : 61.4\n",
      "\n",
      "inaccessible:second-order : 37.1\n",
      "\n",
      "inaccessible:second-order:acyclic : 37.5\n",
      "\n",
      "inaccessible:second-order:cyclic : 36.7\n",
      "\n",
      "inaccessible:set:ALL_character : 64.3\n",
      "\n",
      "inaccessible:set:character_answer_consistency : 80.5\n",
      "\n",
      ">>>>> Dumped evaluation outputs and the report at /import/ravel/1/z5517269/latentqa/baselines/tabel2/CoT/FanToM/data/results!\n",
      ">>>>> Evaluated model responses filename: evaluated_responses_full_input_FANTOM FT Llama-3 3B_cot-True.json\n",
      ">>>>> REPORT filename: REPORT_full_input_FANTOM FT Llama-3 3B_cot-True.json\n"
     ]
    }
   ],
   "source": [
    "!python eval_fantom.py --model 'FANTOM FT Llama-3 3B' --conversation-input-type 'full' --use-cot True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "Already built at data/fantom. version 1.0\n",
      "Generating responses...\n",
      "File /import/ravel/1/z5517269/latentqa/baselines/tabel2/CoT/FanToM/data/results/model_responses_full_input_FANTOM FT Llama-3 8B_cot-True.jsonl exists. Reading responses from file...\n",
      "100%|███████████████████████████████████| 3795/3795 [00:00<00:00, 113265.19it/s]\n",
      "Running evaluation...\n",
      "100%|███████████████████████████████████████| 3795/3795 [01:33<00:00, 40.57it/s]\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "\n",
      "[[ FANToM input type: full ]]\n",
      "[[ Model: FANTOM FT Llama-3 8B ]]\n",
      "\n",
      "model : FANTOM FT Llama-3 8B\n",
      "\n",
      "conversation_input_type : full\n",
      "\n",
      "inaccessible:set:ALL* : 12.8\n",
      "\n",
      "inaccessible:set:ALL : 18.3\n",
      "\n",
      "inaccessible:belief:multiple-choice : 38.800000000000004\n",
      "\n",
      "inaccessible:belief:distance : 55.900000000000006\n",
      "\n",
      "inaccessible:belief_true_word-f1 : 81.10000000000001\n",
      "\n",
      "inaccessible:answerability:set:ALL : 60.3\n",
      "\n",
      "inaccessible:answerability:list : 72.89999999999999\n",
      "\n",
      "inaccessible:answerability:binary-f1 : 93.8\n",
      "\n",
      "inaccessible:info_accessibility:set:ALL : 62.8\n",
      "\n",
      "inaccessible:info_accessibility:list : 73.9\n",
      "\n",
      "inaccessible:info_accessibility:binary-f1 : 93.89999999999999\n",
      "\n",
      "fact_word-f1 : 56.3\n",
      "\n",
      "inaccessible:tom:lists:wrong_reasons:freq : {'included_unaware_character': 33, 'excluded_aware_character': 23, 'did_both': 3}\n",
      "\n",
      "inaccessible:tom:binary:wrong_reasons:freq : {'false_negative': 45, 'false_positive': 64, 'irrelevant_response': 4}\n",
      "\n",
      "inaccessible:first-order : 51.9\n",
      "\n",
      "inaccessible:second-order : 63.9\n",
      "\n",
      "inaccessible:second-order:acyclic : 68.8\n",
      "\n",
      "inaccessible:second-order:cyclic : 59.199999999999996\n",
      "\n",
      "inaccessible:set:ALL_character : 69.5\n",
      "\n",
      "inaccessible:set:character_answer_consistency : 87.8\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/import/ravel/1/z5517269/latentqa/baselines/tabel2/CoT/FanToM/eval_fantom.py\", line 756, in <module>\n",
      "    main(args)\n",
      "  File \"/import/ravel/1/z5517269/latentqa/baselines/tabel2/CoT/FanToM/eval_fantom.py\", line 721, in main\n",
      "    evaluator.run()\n",
      "  File \"/import/ravel/1/z5517269/latentqa/baselines/tabel2/CoT/FanToM/eval_fantom.py\", line 708, in run\n",
      "    self.dump_report_outputs(reports, evaluated_outputs)\n",
      "  File \"/import/ravel/1/z5517269/latentqa/baselines/tabel2/CoT/FanToM/eval_fantom.py\", line 435, in dump_report_outputs\n",
      "    with open(os.path.join(EVAL_DIR_PATH, evaluated_responses_filename), 'w') as f:\n",
      "OSError: [Errno 122] Disk quota exceeded\n"
     ]
    }
   ],
   "source": [
    "!python eval_fantom.py --model 'FANTOM FT Llama-3 8B' --conversation-input-type 'full' --use-cot True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already built at data/fantom. version 1.0\n",
      "Generating responses...\n",
      "File /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results/model_responses_full_input_FANTOM Middle Llama-3 3B_cot-True.jsonl exists. Reading responses from file...\n",
      "100%|███████████████████████████████████| 3795/3795 [00:00<00:00, 124690.64it/s]\n",
      "Running evaluation...\n",
      "100%|██████████████████████████████████████| 3795/3795 [00:11<00:00, 336.79it/s]\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "\n",
      "[[ FANToM input type: full ]]\n",
      "[[ Model: FANTOM Middle Llama-3 3B ]]\n",
      "\n",
      "model : FANTOM Middle Llama-3 3B\n",
      "\n",
      "conversation_input_type : full\n",
      "\n",
      "inaccessible:set:ALL* : 11.899999999999999\n",
      "\n",
      "inaccessible:set:ALL : 25.1\n",
      "\n",
      "inaccessible:belief:multiple-choice : 51.7\n",
      "\n",
      "inaccessible:belief:distance : 46.5\n",
      "\n",
      "inaccessible:belief_true_word-f1 : 72.2\n",
      "\n",
      "inaccessible:answerability:set:ALL : 64.4\n",
      "\n",
      "inaccessible:answerability:list : 76.6\n",
      "\n",
      "inaccessible:answerability:binary-f1 : 92.60000000000001\n",
      "\n",
      "inaccessible:info_accessibility:set:ALL : 63.800000000000004\n",
      "\n",
      "inaccessible:info_accessibility:list : 75.2\n",
      "\n",
      "inaccessible:info_accessibility:binary-f1 : 93.0\n",
      "\n",
      "fact_word-f1 : 44.3\n",
      "\n",
      "inaccessible:tom:lists:wrong_reasons:freq : {'included_unaware_character': 30, 'excluded_aware_character': 17, 'did_both': 4}\n",
      "\n",
      "inaccessible:tom:binary:wrong_reasons:freq : {'false_negative': 39, 'false_positive': 89, 'irrelevant_response': 5}\n",
      "\n",
      "inaccessible:first-order : 37.6\n",
      "\n",
      "inaccessible:second-order : 63.9\n",
      "\n",
      "inaccessible:second-order:acyclic : 66.7\n",
      "\n",
      "inaccessible:second-order:cyclic : 61.199999999999996\n",
      "\n",
      "inaccessible:set:ALL_character : 69.1\n",
      "\n",
      "inaccessible:set:character_answer_consistency : 90.9\n",
      "\n",
      ">>>>> Dumped evaluation outputs and the report at /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results!\n",
      ">>>>> Evaluated model responses filename: evaluated_responses_full_input_FANTOM Middle Llama-3 3B_cot-True.json\n",
      ">>>>> REPORT filename: REPORT_full_input_FANTOM Middle Llama-3 3B_cot-True.json\n"
     ]
    }
   ],
   "source": [
    "!python eval_fantom.py --model 'FANTOM Middle Llama-3 3B' --conversation-input-type 'full' --use-cot True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already built at data/fantom. version 1.0\n",
      "Generating responses...\n",
      "File /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results/model_responses_full_input_FANTOM Middle Llama-3 8B_cot-True.jsonl exists. Reading responses from file...\n",
      "100%|███████████████████████████████████| 3795/3795 [00:00<00:00, 125579.95it/s]\n",
      "Running evaluation...\n",
      "100%|██████████████████████████████████████| 3795/3795 [00:11<00:00, 339.84it/s]\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "\n",
      "[[ FANToM input type: full ]]\n",
      "[[ Model: FANTOM Middle Llama-3 8B ]]\n",
      "\n",
      "model : FANTOM Middle Llama-3 8B\n",
      "\n",
      "conversation_input_type : full\n",
      "\n",
      "inaccessible:set:ALL* : 16.400000000000002\n",
      "\n",
      "inaccessible:set:ALL : 22.8\n",
      "\n",
      "inaccessible:belief:multiple-choice : 49.7\n",
      "\n",
      "inaccessible:belief:distance : 66.10000000000001\n",
      "\n",
      "inaccessible:belief_true_word-f1 : 79.0\n",
      "\n",
      "inaccessible:answerability:set:ALL : 67.60000000000001\n",
      "\n",
      "inaccessible:answerability:list : 73.4\n",
      "\n",
      "inaccessible:answerability:binary-f1 : 94.19999999999999\n",
      "\n",
      "inaccessible:info_accessibility:set:ALL : 61.5\n",
      "\n",
      "inaccessible:info_accessibility:list : 71.6\n",
      "\n",
      "inaccessible:info_accessibility:binary-f1 : 92.80000000000001\n",
      "\n",
      "fact_word-f1 : 51.1\n",
      "\n",
      "inaccessible:tom:lists:wrong_reasons:freq : {'included_unaware_character': 37, 'excluded_aware_character': 20, 'did_both': 1}\n",
      "\n",
      "inaccessible:tom:binary:wrong_reasons:freq : {'false_negative': 22, 'false_positive': 94}\n",
      "\n",
      "inaccessible:first-order : 64.0\n",
      "\n",
      "inaccessible:second-order : 70.1\n",
      "\n",
      "inaccessible:second-order:acyclic : 75.0\n",
      "\n",
      "inaccessible:second-order:cyclic : 65.3\n",
      "\n",
      "inaccessible:set:ALL_character : 73.1\n",
      "\n",
      "inaccessible:set:character_answer_consistency : 91.60000000000001\n",
      "\n",
      ">>>>> Dumped evaluation outputs and the report at /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results!\n",
      ">>>>> Evaluated model responses filename: evaluated_responses_full_input_FANTOM Middle Llama-3 8B_cot-True.json\n",
      ">>>>> REPORT filename: REPORT_full_input_FANTOM Middle Llama-3 8B_cot-True.json\n"
     ]
    }
   ],
   "source": [
    "!python eval_fantom.py --model 'FANTOM Middle Llama-3 8B' --conversation-input-type 'full' --use-cot True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already built at data/fantom. version 1.0\n",
      "Generating responses...\n",
      "File /import/ravel/1/z5517269/latentqa/baselines/fantom/data/results/model_responses_full_input_gpt-4o-mini_cot-True.jsonl exists. Reading responses from file...\n",
      "100%|█████████████████████████████████████| 3795/3795 [2:01:17<00:00,  1.92s/it]\n",
      "Running evaluation...\n",
      "100%|██████████████████████████████████████| 3795/3795 [00:11<00:00, 335.85it/s]\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "\n",
      "[[ FANToM input type: full ]]\n",
      "[[ Model