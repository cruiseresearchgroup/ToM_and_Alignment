{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "# Setting environment variables using os.environ\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12348\"\n",
    "os.environ[\"RAYON_NUM_THREADS\"] = \"48\"\n",
    "os.environ[\"HF_HOME\"] = \"/srv/scratch/CRUISE/Mehdi/HF\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from dataclasses import fields\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from peft import LoraConfig\n",
    "from lit.configs.train_config import train_config\n",
    "from lit.configs.peft_config import lora_config\n",
    "from lit.utils.ToM_reading_utils import get_NegotiationToM_text\n",
    "from lit.utils.infra_utils import (\n",
    "    get_logger, get_ema, update_ema, update_config,\n",
    "    get_tokenizer, get_model\n",
    ")\n",
    "from lit.utils.activation_utils import latent_qa\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/unsloth/__init__.py:44: UserWarning: Unsloth: 'CUDA_VISIBLE_DEVICES' is currently GPU-416d5a5f-1d6c-05e5-bc7b-c89820704bc6 \n",
      "Unsloth currently does not support multi GPU setups - but we are working on it!\n",
      "Multiple CUDA devices detected but we require a single device.\n",
      "We will override CUDA_VISIBLE_DEVICES to first device: GPU-416d5a5f-1d6c-05e5-bc7b-c89820704bc6.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA L40S. Max memory: 44.422 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "target_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W131 11:45:21.327004684 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [localhost]:12348 (errno: 97 - Address family not supported by protocol).\n",
      "[\u001b[34m2025-01-31 11:45:21\u001b[0m] Experiment directory created at /srv/scratch/CRUISE/Mehdi/out/runs-2/085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Starting rank=0, seed=42, world_size=1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88d9edac854475da07a3c6e6bb5736d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eac4f16b96d47aebedb4e97f818ea31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1/3, batch 1002/1335 completed (loss: 5.3210954666137695):  75%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  \u001b[0m| 1002/1335 [01:36<00:32, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value of the evaluation epoch is 3863.80078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1/3, batch 1335/1335 completed (loss: 3.4494495391845703): : 667it [00:46, 14.32it/s]\n",
      "Training Epoch: 2/3, batch 669/1335 completed (loss: 9.436586380004883):  50%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     \u001b[0m| 669/1335 [01:04<01:04, 10.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value of the evaluation epoch is 3863.7998046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2/3, batch 1335/1335 completed (loss: 10.768994331359863): : 1000it [01:18, 12.72it/s]\n",
      "Training Epoch: 3/3, batch 336/1335 completed (loss: 13.304550170898438):  25%|\u001b[34mâ–ˆâ–ˆâ–Œ       \u001b[0m| 336/1335 [00:32<01:36, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value of the evaluation epoch is 3863.800048828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3/3, batch 1335/1335 completed (loss: 12.943473815917969): : 1333it [01:51, 12.00it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1522a0d1b943aeb622db484f41be70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/711 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference :   0%|\u001b[31m          \u001b[0m| 0/711 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 135\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataloader):\n\u001b[1;32m    134\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {key: torch\u001b[38;5;241m.\u001b[39mtensor(val)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 135\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m)\n\u001b[1;32m    136\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_text)\n",
      "File \u001b[0;32m/srv/scratch/CRUISE/z5517269/miniconda/envs/latentqa/lib/python3.11/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'module'"
     ]
    }
   ],
   "source": [
    "kwargs = {}\n",
    "# Distributed Training Setup\n",
    "dist.init_process_group(\"nccl\")\n",
    "assert torch.cuda.is_available()\n",
    "args = train_config()\n",
    "update_config(args, **kwargs)\n",
    "\n",
    "fsdp_args = None\n",
    "if args.use_fsdp:\n",
    "    from lit.configs.fsdp_config import fsdp_config\n",
    "    fsdp_args = fsdp_config()\n",
    "    update_config(fsdp_args, **kwargs)\n",
    "print(fsdp_args)\n",
    "\n",
    "rank = dist.get_rank()\n",
    "device = rank % torch.cuda.device_count()\n",
    "torch.cuda.set_device(device)\n",
    "torch.cuda.empty_cache()\n",
    "seed = args.seed * dist.get_world_size() + rank\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "print(f\"Starting rank={rank}, seed={seed}, world_size={dist.get_world_size()}.\")\n",
    "logger = get_logger(args, rank)\n",
    "\n",
    "# Tokenizer and Dataset Preparation\n",
    "args.target_model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "args.eval_every_n_steps = 3*334\n",
    "args.num_epochs = 3\n",
    "# tokenizer = get_tokenizer(args.target_model_name)\n",
    "\n",
    "def tokenize_data(example):\n",
    "    prompt = f\"Question: {example['input']}\\nAnswer: {example['output']}\"\n",
    "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=750, return_tensors=\"pt\")\n",
    "\n",
    "args.train_qa = \"../../data/NegotiationToM/train.json\"\n",
    "args.eval_qa = \"../../data/NegotiationToM/valid.json\"\n",
    "args.is_steer = False\n",
    "\n",
    "# Training Dataset\n",
    "read_prompts, QAs = get_NegotiationToM_text(args, tokenizer, True)\n",
    "data = [{\n",
    "    'input': str(rp+[qa[0]]),\n",
    "    'output': '=> '+ qa[1]['content']\n",
    "} for rp, qa in zip(read_prompts, QAs)]\n",
    "dataset = Dataset.from_list(data).map(tokenize_data, batched=False)\n",
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Evaluation Dataset\n",
    "read_prompts, QAs = get_NegotiationToM_text(args, tokenizer, False)\n",
    "data = [{\n",
    "    'input': str(rp+[qa[0]]),\n",
    "    'output': '=> '+ qa[1]['content']\n",
    "} for rp, qa in zip(read_prompts, QAs)]\n",
    "dataset = Dataset.from_list(data).map(tokenize_data, batched=False)\n",
    "eval_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Model Initialization\n",
    "lora_params = {k.name: getattr(lora_config(), k.name) for k in fields(lora_config())}\n",
    "peft_config = LoraConfig(**lora_params)\n",
    "# target_model = get_model(\n",
    "#     args.target_model_name,\n",
    "#     peft_config=peft_config,\n",
    "#     fsdp_args=fsdp_args,\n",
    "#     device=device,\n",
    "#     rank=rank,\n",
    "#     distributed_training=True,\n",
    "# )\n",
    "# target_model = model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Exponential Moving Average\n",
    "ema = get_ema(target_model, decay=args.ema_decay, device=device)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = optim.AdamW(target_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "training_steps = len(train_dataloader) * args.num_epochs\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=training_steps)\n",
    "\n",
    "# Training Loop\n",
    "train_steps = 0\n",
    "for epoch in range(args.num_epochs):\n",
    "    target_model.train()\n",
    "    total_length = len(train_dataloader) // args.gradient_accumulation_steps\n",
    "    pbar = tqdm(colour=\"blue\", desc=f\"Training Epoch: {epoch+1}\", total=total_length, dynamic_ncols=True)\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        inputs = {key: torch.tensor(val).squeeze(1).to(device) for key, val in batch.items() if key in [\"input_ids\", \"attention_mask\"]}\n",
    "        outputs = target_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        \n",
    "        loss = outputs.loss / args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        train_steps += 1\n",
    "        \n",
    "        if train_steps % args.gradient_accumulation_steps == 0:\n",
    "            if args.gradient_clipping and args.gradient_clipping_threshold > 0.0:\n",
    "                torch.nn.utils.clip_grad_norm_(target_model.parameters(), args.gradient_clipping_threshold)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            update_ema(ema, target_model, decay=args.ema_decay)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        pbar.set_description(f\"Training Epoch: {epoch+1}/{args.num_epochs}, batch {step+1}/{len(train_dataloader)} completed (loss: {loss.detach().float()})\")\n",
    "        \n",
    "        # Periodic Evaluation\n",
    "        if args.eval_ppl and train_steps % args.eval_every_n_steps == 0:\n",
    "            total_loss = 0.0\n",
    "            pbar = tqdm(colour=\"green\", desc=f\"Evaluating Epoch: {epoch+1}\", total=len(eval_dataloader), dynamic_ncols=True)\n",
    "            \n",
    "            for step, batch in enumerate(eval_dataloader):\n",
    "                inputs = {key: torch.tensor(val).squeeze(1).to(device) for key, val in batch.items() if key in [\"input_ids\", \"attention_mask\"]}\n",
    "                outputs = target_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                total_loss += outputs.loss.detach().float()\n",
    "                pbar.update(1)\n",
    "            print(f\"The loss value of the evaluation epoch is {total_loss}\")\n",
    "    \n",
    "    # End of epoch\n",
    "    scheduler.step()\n",
    "    pbar.close()\n",
    "\n",
    "# Test Dataset\n",
    "args.eval_qa = \"../../data/NegotiationToM/test.json\"\n",
    "read_prompts, QAs = get_NegotiationToM_text(args, tokenizer, False)\n",
    "data = [{\n",
    "    'input': str(rp+[qa[0]]),\n",
    "    'output': '=> '\n",
    "} for rp, qa in zip(read_prompts, QAs)]\n",
    "dataset = Dataset.from_list(data).map(tokenize_data, batched=False)\n",
    "test_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "pbar = tqdm(colour=\"red\", desc=f\"Inference : \", total=len(test_dataloader), dynamic_ncols=True)\n",
    "\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    inputs = {key: torch.tensor(val).squeeze(1).to(device) for key, val in batch.items() if key in [\"input_ids\", \"attention_mask\"]}\n",
    "    output_ids = target_model.module.generate(inputs[\"input_ids\"], max_new_tokens=150)\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(\"Generated Text:\", generated_text)\n",
    "    print('\\nThis is the end of the current sample\\n')\n",
    "    pbar.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"[{'role': 'Agent 2', 'content': 'Hello. How are you?'}, {'role': 'Agent 1', 'content': 'I am good. I am pretty excited for the trip this weekend. what about you?'}, {'role': 'Agent 2', 'content': 'Very excited. It will be fun.'}, {'role': 'Agent 1', 'content': 'Yes, Hopefully the weather holds up. So I was thinking, i am bringning my two kids along with me, and they will need food, so I was thinking i could take the three foods, and either one of the firewood or water, up to you.'}, {'role': 'Agent 2', 'content': 'I would also like a little extra food for my kids. Maybe we can split it somehow?'}, {'role': 'Agent 1', 'content': 'Ok, I am willing to give you one food, in exchange for two firewoods, that would mean you get 3 waters, 1 food and 1 firewood. you get 5 items, while i get 4.'}, {'role': 'user', 'content': 'what is the intent of each agent for the last utterances? What are the beliefs and desires of each agent?'}]\", 'output': \"{'role': 'assistant', 'content': 'The intent of the Agent 1 is [No-Intention] and the intent of the Agent 2 is [Describe-Need,Promote-Coordination] Regarding the Agent 1, Desire High: Food, Desire Medium: Firewood, Desire Low: Water,  Belief High: Food, Belief Medium: Not Given, Belief Low: Not Given. Regarding the Agent 2, Desire High: Food, Desire Medium: Not Given, Desire Low: Not Given,  Belief High: Food, Belief Medium: Firewood, Belief Low: Water.'}\"}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_data(example):\n",
    "    prompt = f\"Question: {example['input']}\\nAnswer: \"\n",
    "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=1000, return_tensors=\"pt\")\n",
    "\n",
    "args.eval_qa = \"../../data/NegotiationToM/test.json\"\n",
    "read_prompts, QAs = get_NegotiationToM_text(args, tokenizer, False)\n",
    "data = [{\n",
    "    'input': str(rp+[qa[0]]),\n",
    "    'output': str(qa[1])\n",
    "} for rp, qa in zip(read_prompts, QAs)]\n",
    "\n",
    "data[0]\n",
    "print(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
