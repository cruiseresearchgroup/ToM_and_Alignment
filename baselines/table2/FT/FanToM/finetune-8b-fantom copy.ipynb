{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a9ff04-34d8-4798-84c3-f1a06370dd8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T21:04:39.870587Z",
     "iopub.status.busy": "2025-02-07T21:04:39.870171Z",
     "iopub.status.idle": "2025-02-07T21:07:30.114163Z",
     "shell.execute_reply": "2025-02-07T21:07:30.113394Z",
     "shell.execute_reply.started": "2025-02-07T21:04:39.870560Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02644e28-0b12-473a-939c-5358586ad30b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T04:10:41.586291Z",
     "iopub.status.busy": "2025-02-08T04:10:41.585738Z",
     "iopub.status.idle": "2025-02-08T04:10:53.555293Z",
     "shell.execute_reply": "2025-02-08T04:10:53.554717Z",
     "shell.execute_reply.started": "2025-02-08T04:10:41.586272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2500 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa606350-0a68-429d-913f-0d0d83007e94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T04:10:53.556429Z",
     "iopub.status.busy": "2025-02-08T04:10:53.556108Z",
     "iopub.status.idle": "2025-02-08T04:11:08.309398Z",
     "shell.execute_reply": "2025-02-08T04:11:08.308615Z",
     "shell.execute_reply.started": "2025-02-08T04:10:53.556412Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e624f44-fd93-4f49-a7d6-f86bcfb05018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T04:11:08.310575Z",
     "iopub.status.busy": "2025-02-08T04:11:08.310312Z",
     "iopub.status.idle": "2025-02-08T04:11:08.322533Z",
     "shell.execute_reply": "2025-02-08T04:11:08.322060Z",
     "shell.execute_reply.started": "2025-02-08T04:11:08.310558Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "def set_beliefQA_multiple_choices(qa):\n",
    "    if qa['question_type'].endswith(\":inaccessible\"):\n",
    "        option_a = qa['wrong_answer']\n",
    "        option_b = qa['correct_answer']\n",
    "    else:\n",
    "        option_a = qa['wrong_answer']\n",
    "        option_b = qa['correct_answer']\n",
    "\n",
    "    answer_goes_last = random.choice([True, False])\n",
    "    if answer_goes_last:\n",
    "        choices = [option_a, option_b]\n",
    "        answer = 1\n",
    "    else:\n",
    "        choices = [option_b, option_a]\n",
    "        answer = 0\n",
    "\n",
    "    # option letters iterate over the alphabet\n",
    "    option_letters = [\"(\" + chr(x) + \")\" for x in range(ord('a'), len(choices) + ord('a'))]\n",
    "    choices_text = \"\"\n",
    "    for letter, option in zip(option_letters, choices):\n",
    "        choices_text += \"{} {}\\n\".format(letter, option)\n",
    "\n",
    "    return choices_text, answer\n",
    "\n",
    "def setup_fantom(df, conversation_input_type='full'):\n",
    "    total_num_q = 0\n",
    "    for idx, _set in df.iterrows():\n",
    "        total_num_q += len(_set['beliefQAs'])\n",
    "        total_num_q += len(_set['answerabilityQAs_binary'])\n",
    "        total_num_q += len(_set['infoAccessibilityQAs_binary'])\n",
    "        if _set['factQA'] is not None:\n",
    "            total_num_q += 1\n",
    "        if _set['answerabilityQA_list'] is not None:\n",
    "            total_num_q += 1\n",
    "        if _set['infoAccessibilityQA_list'] is not None:\n",
    "            total_num_q += 1\n",
    "\n",
    "    inputs = []\n",
    "    qas = []\n",
    "    for idx, _set in df.iterrows():\n",
    "        if conversation_input_type == \"short\":\n",
    "            context = _set['short_context'].strip()\n",
    "        elif conversation_input_type == \"full\":\n",
    "            context = _set['full_context'].strip()\n",
    "        \n",
    "        set_id = _set['set_id']\n",
    "        fact_q = _set['factQA']['question']\n",
    "        fact_a = _set['factQA']['correct_answer']\n",
    "\n",
    "        # Fact Question\n",
    "        _set['factQA']['context'] = context\n",
    "        input_text = \"{}\\n\\nQuestion: {}\\nAnswer:\".format(context, fact_q)\n",
    "        _set['factQA']['input_text'] = input_text\n",
    "        _set['factQA']['set_id'] = set_id\n",
    "        qas.append(_set['factQA'])\n",
    "        inputs.append(input_text)\n",
    "\n",
    "        for _belief_qa in _set['beliefQAs']:\n",
    "            # Belief Questions\n",
    "            _belief_qa['context'] = context\n",
    "            input_text = \"{}\\n\\nQuestion: {}\\nAnswer:\".format(context, _belief_qa['question'])\n",
    "            _belief_qa['input_text'] = input_text\n",
    "            _belief_qa['set_id'] = set_id\n",
    "            qas.append(_belief_qa)\n",
    "            inputs.append(input_text)\n",
    "\n",
    "            # Multiple Choice Belief Questions\n",
    "            _mc_belief_qa = {**_belief_qa}\n",
    "            choices_text, answer = set_beliefQA_multiple_choices(_mc_belief_qa)\n",
    "            mc_question = \"{}\\n{}\\n\\nChoose an answer from above:\".format(_belief_qa['question'], choices_text.strip())\n",
    "            _mc_belief_qa['question'] = mc_question\n",
    "            _mc_belief_qa['question_type'] = _mc_belief_qa['question_type'] + \":multiple-choice\"\n",
    "            _mc_belief_qa['choices_text'] = choices_text\n",
    "            _mc_belief_qa['choices_list'] = choices_text.strip().split(\"\\n\")\n",
    "            _mc_belief_qa['correct_answer'] = answer\n",
    "            input_text = \"{}\\n\\nQuestion: {}\".format(context, mc_question)\n",
    "            _mc_belief_qa['input_text'] = input_text\n",
    "            qas.append(_mc_belief_qa)\n",
    "            inputs.append(input_text)\n",
    "\n",
    "        # Answerability List Questions\n",
    "        _set['answerabilityQA_list']['fact_question'] = fact_q\n",
    "        _set['answerabilityQA_list']['context'] = context\n",
    "        input_text = \"{}\\n\\nTarget: {}\\nQuestion: {}\\nAnswer:\".format(context, fact_q, _set['answerabilityQA_list']['question'])\n",
    "        _set['answerabilityQA_list']['input_text'] = input_text\n",
    "        _set['answerabilityQA_list']['set_id'] = set_id\n",
    "        if conversation_input_type == \"full\" and len(_set['answerabilityQA_list']['wrong_answer']) > 0:\n",
    "            _set['answerabilityQA_list']['missed_info_accessibility'] = 'inaccessible'\n",
    "        qas.append(_set['answerabilityQA_list'])\n",
    "        inputs.append(input_text)\n",
    "\n",
    "        # Answerability Binary Questions\n",
    "        if conversation_input_type == \"full\":\n",
    "            missed_info_accessibility_for_full = _set['answerabilityQAs_binary'][0]['missed_info_accessibility']\n",
    "            for _info_accessibility_qa in _set['answerabilityQAs_binary']:\n",
    "                if _info_accessibility_qa['correct_answer'] != \"yes\":\n",
    "                    missed_info_accessibility_for_full = 'inaccessible'\n",
    "\n",
    "        for _answerability_qa in _set['answerabilityQAs_binary']:\n",
    "            _answerability_qa['fact_question'] = fact_q\n",
    "            _answerability_qa['context'] = context\n",
    "            input_text = \"{}\\n\\nTarget: {}\\nQuestion: {} Answer yes or no.\\nAnswer:\".format(context, fact_q, _answerability_qa['question'])\n",
    "            _answerability_qa['input_text'] = input_text\n",
    "            _answerability_qa['set_id'] = set_id\n",
    "            if conversation_input_type == \"full\":\n",
    "                _answerability_qa['missed_info_accessibility'] = missed_info_accessibility_for_full\n",
    "            qas.append(_answerability_qa)\n",
    "            inputs.append(input_text)\n",
    "\n",
    "        # Info Accessibility List Questions\n",
    "        _set['infoAccessibilityQA_list']['fact_question'] = fact_q\n",
    "        _set['infoAccessibilityQA_list']['fact_answer'] = fact_a\n",
    "        _set['infoAccessibilityQA_list']['context'] = context\n",
    "        input_text = \"{}\\n\\nInformation: {} {}\\nQuestion: {}\\nAnswer:\".format(context, fact_q, fact_a, _set['infoAccessibilityQA_list']['question'])\n",
    "        _set['infoAccessibilityQA_list']['input_text'] = input_text\n",
    "        _set['infoAccessibilityQA_list']['set_id'] = set_id\n",
    "        if conversation_input_type == \"full\" and len(_set['infoAccessibilityQA_list']['wrong_answer']) > 0:\n",
    "            _set['infoAccessibilityQA_list']['missed_info_accessibility'] = 'inaccessible'\n",
    "        qas.append(_set['infoAccessibilityQA_list'])\n",
    "        inputs.append(input_text)\n",
    "\n",
    "        # Info Accessibility Binary Questions\n",
    "        if conversation_input_type == \"full\":\n",
    "            missed_info_accessibility_for_full = _set['infoAccessibilityQAs_binary'][0]['missed_info_accessibility']\n",
    "            for _info_accessibility_qa in _set['infoAccessibilityQAs_binary']:\n",
    "                if _info_accessibility_qa['correct_answer'] != \"yes\":\n",
    "                    missed_info_accessibility_for_full = 'inaccessible'\n",
    "\n",
    "        for _info_accessibility_qa in _set['infoAccessibilityQAs_binary']:\n",
    "            _info_accessibility_qa['fact_question'] = fact_q\n",
    "            _info_accessibility_qa['fact_answer'] = fact_a\n",
    "            _info_accessibility_qa['context'] = context\n",
    "            input_text = \"{}\\n\\nInformation: {} {}\\nQuestion: {} Answer yes or no.\\nAnswer:\".format(context, fact_q, fact_a, _info_accessibility_qa['question'])\n",
    "            _info_accessibility_qa['input_text'] = input_text\n",
    "            _info_accessibility_qa['set_id'] = set_id\n",
    "            if conversation_input_type == \"full\":\n",
    "                _info_accessibility_qa['missed_info_accessibility'] = missed_info_accessibility_for_full\n",
    "            qas.append(_info_accessibility_qa)\n",
    "            inputs.append(input_text)\n",
    "\n",
    "    return inputs, qas\n",
    "\n",
    "\n",
    "def get_FanToM_text(train_config, tokenizer, train=True):\n",
    "    data_path = train_config.train_qa if train else train_config.eval_qa\n",
    "    df = pd.read_json(data_path)\n",
    "    metas, QAs = setup_fantom(df)\n",
    "    final_QAs, read_prompts = [], []\n",
    "    for item, meta in zip(QAs, metas):\n",
    "        final_QAs.append([\n",
    "                {\"role\": \"user\", \"content\": item['input_text'].split('\\n\\n')[1].strip('\\nAnswer:')},\n",
    "                {\"role\": \"assistant\", \"content\": str(item['correct_answer'])}\n",
    "        ])\n",
    "        \n",
    "        temp_read_prompt = []\n",
    "        for line in item['input_text'].split('\\n\\n')[0].strip().split('\\n'):\n",
    "            temp_read_prompt.append({\n",
    "                'role':line.split(':')[0].strip(),\n",
    "                'content':line.split(':')[1].strip(),\n",
    "            })\n",
    "        read_prompts.append(temp_read_prompt)\n",
    "    assert len(final_QAs)==len(read_prompts)\n",
    "    return read_prompts, final_QAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762cf1c8-5e02-4f38-b95d-1d2b9a028d71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T04:11:08.323661Z",
     "iopub.status.busy": "2025-02-08T04:11:08.323392Z",
     "iopub.status.idle": "2025-02-08T04:11:08.985163Z",
     "shell.execute_reply": "2025-02-08T04:11:08.984562Z",
     "shell.execute_reply.started": "2025-02-08T04:11:08.323647Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "alpaca_prompt = \"\"\"\n",
    "### Dialogue:\n",
    "{}\n",
    "\n",
    "### Qeustion:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\".strip()\n",
    "class dummy_config:\n",
    "    def __init__(self):\n",
    "        self.train_qa = './FanToM/train.json'\n",
    "        self.eval_qa = './FanToM/valid.json'\n",
    "train_read_prompts, train_QAs = get_FanToM_text(dummy_config(), tokenizer, True)\n",
    "flat_dialog = lambda dialog:' '.join([f'{item[\"role\"]} : {item[\"content\"]}' for item in dialog])\n",
    "flat_question = lambda question:question['content'].replace('Question: ', '')\n",
    "flat_answer = lambda answer:answer['content']\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "train_data = [\n",
    "    alpaca_prompt.format(flat_dialog(rp), flat_question(qa[0]), flat_answer(qa[1])) + ' ' + EOS_TOKEN\n",
    "    for rp, qa in zip(train_read_prompts, train_QAs)]\n",
    "\n",
    "train_formatted_data = {'text':train_data}\n",
    "train_dataset = Dataset.from_dict(train_formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a671fec3-5095-4ab6-b6cd-24c7a00fb709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T04:11:08.985883Z",
     "iopub.status.busy": "2025-02-08T04:11:08.985724Z",
     "iopub.status.idle": "2025-02-08T04:11:09.151795Z",
     "shell.execute_reply": "2025-02-08T04:11:09.151252Z",
     "shell.execute_reply.started": "2025-02-08T04:11:08.985868Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_read_prompts, test_QAs = get_FanToM_text(dummy_config(), tokenizer, False)\n",
    "test_data = [\n",
    "    alpaca_prompt.format(flat_dialog(rp), flat_question(qa[0]), flat_answer(qa[1])) + ' ' + EOS_TOKEN\n",
    "    for rp, qa in zip(test_read_prompts, test_QAs)]\n",
    "\n",
    "test_formatted_data = {'text':test_data}\n",
    "test_dataset = Dataset.from_dict(test_formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d57ae4a0-289f-45e2-ac19-9efb7f6eba42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T04:11:09.152615Z",
     "iopub.status.busy": "2025-02-08T04:11:09.152317Z",
     "iopub.status.idle": "2025-02-08T04:11:09.157614Z",
     "shell.execute_reply": "2025-02-08T04:11:09.157187Z",
     "shell.execute_reply.started": "2025-02-08T04:11:09.152600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Dialogue:\\nJorge : Hey Gunner, have you ever tried any of the modern-day food trends? They are pretty controversial sometimes. Gunner : You bet, Jorge. I have tried the plant-based diet trend. Honestly, it's cool how our food preferences can significantly reduce our carbon footprint. Jorge : That's true! It is much more than just a diet trend; it is an environmental responsibility as well. How about any controversial food experiences? Gunner : I once tried the raw food diet, which is quite tricky. I mean, consuming only uncooked and unprocessed food is quite a switch. It's disputable because many say cooking improves digestion and increases nutrient absorption. Jorge : I agree, I think even our ancestors discovered fire to cook their food and make it more digestible. Plus, the raw diet can lead to limited protein options and nutritional deficiencies. Gunner : Exactly! Also, another controversial trend I've noticed is this obsession with 'superfoods'. They're suddenly everywhere and some people overly rely on them, thinking they're the solution to all health problems. Jorge : I know, right? It's essential to realize that superfoods are not a one-stop solution for overall health. Moderation and a balanced diet are key. Just like with the gluten-free trend, not everyone needs it, only those with specific health conditions. Gunner : Absolutely! And guess what, I swear by the old saying 'You are what you eat'. But in my opinion, it should be, 'You are healthy if what you eat is balanced and right for you.' Jorge : Well said, Gunner! Also, sustainable food trends like opting for locally sourced, in-season produce and reducing food wastage are things I can get on board with, no controversies there! Gunner : I couldnâ€™t agree more, Jorge! These are not just trends, but necessary steps towards healthier lives and a better environment. Everett : Hey guys, I've been listening in and I agree with you. My doctor recently put me on a low-sodium diet because of my high blood pressure and it's amazing how much that one change has improved my health. Gunner : That's great, Everett! Singular dietary changes can have a big impact, can't they? Jorge : It just highlights how everyone's dietary needs are different. One diet won't work for everyone, it's important to figure out what's best for your body. Everett : Exactly! And while I loved salty snacks, I've discovered new healthier favorites I might never have tried before. So in my case, this diet change was ironically a blessing in disguise. Gunner : This does underline how food habits directly affect our health and general well-being. Whether it's to manage a health condition or become more conscious of our environmental impact. Jorge : You've got a point there, Gunner. I think we're all becoming more conscientious about our food choices, and how they affect not just our individual health, but the health of the planet as well. Jorge : Guys, I need to step away for a moment to stretch and clear my mind. You carry on. Gunner : Sure thing, Jorge. So Everett, since you've made this dietary change, have you discovered any favorite cooking techniques or recipes? Everett : Definitely, Gunner. Since I'm trying to reduce my sodium intake, I've started using much more herbs and spices to season my food. For instance, one of my go-to recipes now is grilled chicken with a homemade rosemary and garlic rub. Gunner : That does sound pretty delicious. I agree, herbs, and spices are such an underrated way to add flavor without the unhealthy aspects. I'm a big fan of roasting vegetables with a sprinkle of herbs. It brings out a sweetness that you wouldn't normally notice. Everett : I hadn't thought of that, Gunner. I've been meaning to experiment more with vegetarian dishes to further improve my health. Any particular recipe or veggies you'd suggest? Gunner : I'd say go colorful. The more colorful your plate, the more nutrients you are getting. I love roasting a mix of carrots, bell peppers and zucchini with some thyme and black pepper. Everett : Sounds really delicious! And you're right, it's a great way to get a mix of different nutrients. Thanks for the suggestion, Gunner! Gunner : You're welcome, Everett! After all, we're all in this journey towards better health together. Feel free to share more of your discoveries in the future, I'm always up for new ideas and recipes! Wesley : Hi guys, sorry for dropping in late. I heard you discussing food trends and dietary changes. I recently developed a fondness for Mediterranean food; it somehow strikes a perfect balance between taste and health. Gunner : True! Mediterranean foods are rich in olive oil, lean meats, fish, whole grains, and lots of fruits and vegetables. It's one of the healthiest diets on the planet. Everett : I couldnâ€™t agree more Wesley. Apart from health benefits, I love the wide variety of flavors that Mediterranean dishes offer. By the way, do you have any favorite Mediterranean dish? Wesley : I absolutely love Greek Salad! It is refreshing and packed with nutrients. How about you, Gunner? Gunner : I am a fan of hummus and pita bread. It's a go-to snack for me when I'm aiming for something healthy yet delicious. Everevt : Thatâ€™s a great choice, Gunner. I love the simplicity of it yet itâ€™s so satisfying. Wesley : On the topic of simple and satisfying, anyone here tried Shakshuka? Itâ€™s essentially eggs poached in a sauce of tomatoes and spices. So simple, yet so flavorful! Gunner : Yes, I've tried Shakshuka and it is indeed a power-packed breakfast. Ideal for those who prefer starting their day with a protein-rich meal. Everett : I am intrigued now. Iâ€™ll definitely try it out as it sounds like a great heart-healthy dish considering my low-sodium diet. Wesley : Trust me, Everett, you will love it! I guess, after all, a healthy diet can really be versatile and delicious. Gunner : Indeed, Wesley. It's all about exploring and experimenting with different cuisines, ingredients, and flavors. We could make this conversation a regular happening to share more ideas and recipes. Everett : Sounds like a plan, Gunner. Sharing is caring, after all. And when it comes to food, there's always something new to learn and explore!\\n\\n### Qeustion:\\nHow did the conversation explore the idea of consuming locally sourced, in-season produce and reducing food wastage in relation to personal health and environmental impacts?\\n\\n### Response:\\nThe conversation explored the idea of consuming locally sourced, in-season produce and reducing food wastage as a sustainable food trend. Jorge suggested that he could get on board with these trends as they were not controversial and were necessary for healthier lives and a better environment. These practices support personal health as they provide fresher, nutrient-rich food options. Environmentally, they help to minimize carbon footprint related to food transportation and reduce waste that exacerbates landfill problems and methane production. The participants agreed they were conscientious about their food choices having an impact on not just their individual health, but the health of the planet as well. <|eot_id|>\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "242d4b50-7c39-4a7f-b558-2107c69992e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T04:11:09.158326Z",
     "iopub.status.busy": "2025-02-08T04:11:09.158081Z",
     "iopub.status.idle": "2025-02-08T04:11:23.166842Z",
     "shell.execute_reply": "2025-02-08T04:11:23.166315Z",
     "shell.execute_reply.started": "2025-02-08T04:11:09.158313Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ae7b46647842d09e7c3281fb173113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/7223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d09ec9b9074e47a875b4c0ed606a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset= test_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=240,\n",
    "        learning_rate=5e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",  # Use this for WandB etc\n",
    "        evaluation_strategy=\"steps\",  # Evaluate at a specific step interval\n",
    "        eval_steps=30,  # Perform evaluation every 15 steps\n",
    "        save_steps=30,  # Optionally, save the model every 15 steps\n",
    "        load_best_model_at_end=True  # Optional if you want the best model based on evaluation\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69cd76c-7cec-48ce-a53d-275e6f7bf7ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T04:11:23.167574Z",
     "iopub.status.busy": "2025-02-08T04:11:23.167398Z",
     "iopub.status.idle": "2025-02-08T06:59:58.361278Z",
     "shell.execute_reply": "2025-02-08T06:59:58.360394Z",
     "shell.execute_reply.started": "2025-02-08T04:11:23.167556Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 7,223 | Num Epochs = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 128 | Total steps = 240\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/240 2:47:00 < 1:37:58, 0.01 it/s, Epoch 2.65/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.124000</td>\n",
       "      <td>1.260737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.659798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.430272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.483304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.545278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:157\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:382\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:68\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.12/site-packages/accelerate/accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e872be-b37e-47d4-aec7-88d37b89c3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T01:32:51.697924Z",
     "iopub.status.busy": "2025-02-08T01:32:51.697542Z",
     "iopub.status.idle": "2025-02-08T03:13:15.260133Z",
     "shell.execute_reply": "2025-02-08T03:13:15.259668Z",
     "shell.execute_reply.started": "2025-02-08T01:32:51.697904Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 7,223 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 128 | Total steps = 180\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 1:39:44, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.411100</td>\n",
       "      <td>1.466766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.781800</td>\n",
       "      <td>1.071449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.672967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>0.501574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.450482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.444764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7ee3b81-9f9a-485c-bfad-ed0d2beb9386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T07:01:07.659341Z",
     "iopub.status.busy": "2025-02-08T07:01:07.658960Z",
     "iopub.status.idle": "2025-02-08T07:01:25.473145Z",
     "shell.execute_reply": "2025-02-08T07:01:25.472361Z",
     "shell.execute_reply.started": "2025-02-08T07:01:07.659326Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "best_model = AutoModelForCausalLM.from_pretrained(\"./outputs/checkpoint-90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40fcda4-bff1-431e-83d3-70564ccf8bb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T07:01:31.653405Z",
     "iopub.status.busy": "2025-02-08T07:01:31.652769Z",
     "iopub.status.idle": "2025-02-08T07:41:13.981665Z",
     "shell.execute_reply": "2025-02-08T07:41:13.981092Z",
     "shell.execute_reply.started": "2025-02-08T07:01:31.653386Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3795/3795 [39:42<00:00,  1.59it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "class dummy_config:\n",
    "    def __init__(self):\n",
    "        self.train_qa = './FanToM/test.json'\n",
    "        self.eval_qa = './FanToM/test.json'\n",
    "read_prompts, QAs = get_FanToM_text(dummy_config(), tokenizer, False)\n",
    "from datasets import Dataset\n",
    "# alpaca_prompt = \"\"\"\n",
    "# ### Input:\n",
    "# {}\n",
    "\n",
    "# ### Response:\n",
    "# {}\"\"\".strip()\n",
    "responses_data = []\n",
    "for idx, (rp, qa) in tqdm(enumerate(list(zip(read_prompts, QAs))), total=len(QAs)):\n",
    "  # inputs = alpaca_prompt.format(str(rp+[qa[0]]), '=> ')\n",
    "  inputs = alpaca_prompt.format(flat_dialog(rp), flat_question(qa[0]), '') \n",
    "  # print(inputs)\n",
    "  inputs = tokenizer([inputs], return_tensors = \"pt\").to(\"cuda\")\n",
    "  outputs = model.generate(**inputs, max_new_tokens = 130, use_cache = True)\n",
    "  response = tokenizer.batch_decode(outputs)\n",
    "  # print(str(response).split('### Response:')[-1].strip())\n",
    "  response = str(response).split('### Response:')[-1].strip().replace('<|eot_id|>\"]', '').replace('\\\\n', '')\n",
    "  # print(response.strip())\n",
    "  temp = {\n",
    "      'index':idx,\n",
    "      'response':response.strip(),\n",
    "      'input_prompt':\"what is the intent of each agent for the last utterances? What are the beliefs and desires of each agent?\",\n",
    "      'ground_truth':qa[-1]['content'].strip(),\n",
    "  }\n",
    "  responses_data.append(temp)\n",
    "\n",
    "with open(\"FanToM FT Llama-3 3B.jsonl\", \"w\") as f:\n",
    "  json.dump(responses_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc808d8-50dd-457a-b3b7-ade16a6f9ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
