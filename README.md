# Theory of Mind for Large Language Model Alignment

This implementation is adopted from the [LatentQA](https://github.com/aypan17/latentqa/tree/main) repository, with modifications to investigate the presence and consistency of Theory of Mind (ToM) in Large Language Models (LLMs).

## Dataset Preparation

This section details how each dataset is prepared for training:

### CaSiNo Dataset

The CaSiNo dataset is used directly without any modifications from the original paper's repository: [link](https://github.com/kushalchawla/CaSiNo).

### CraigslistBargain Dataset

The CraigslistBargain dataset is retrieved from the webpage associated with the paper: [link](https://stanfordnlp.github.io/cocoa/).

### FanToM Dataset

The FanToM dataset is downloaded from the link provided in the paper's repository: [link](https://github.com/skywalker023/fantom/tree/main). This link points to a zip file hosted on Google Drive. After downloading, the dataset is split into training, validation, and test sets using the `train_test_split` function from the `sklearn.model_selection` library. The random state is set to `42` for reproducibility. The split is as follows:

- **Test Set**: 30% of the data is reserved for testing.
- **Train and Validation Sets**: The remaining 70% is split into training and validation sets with an 80:20 ratio.

### Negotiation ToM Dataset

Similar to the FanToM dataset, the Negotiation ToM dataset is downloaded from the paper's repository: [link](https://github.com/HKUST-KnowComp/NegotiationToM) and processed as follows:

1. Download the dataset.
2. Split the data into training, validation, and test sets using the same procedure as the FanToM dataset.

## Training New Decoder Models

To train a new decoder model, it is necessary to modify the configuration files in `lit/configs/*` and run the `train.py` file with the appropriate arguments. 

- All experiments reported in Table 1 of the reference article can be reproduced using `scripts/table1_train_{dataset_name}.sh`.
- Table 2 results are generated using `scripts/table2_train.sh`.
- Training models for steering LLMs is done using `scripts/train_steer.sh`.

For partial training, it is recommended to comment out the unwanted parts in each script and execute them in a suitable environment.

## Reading ToM

After training a new model, follow these steps to read its internal ToM:

1. **Set Model Path**: Specify the path to the trained model in the `run_reading.sh` file.
2. **Configure Output**: Modify the `interpret_config.py` file to:
   - Choose a name for the output file containing responses generated by the decoder model for the test set.
   - Select the dataset.
   - Ensure the output file structure matches the evaluation script requirements for each dataset.

Scripts such as `scripts/table2_reading_NegotiationToM_FanToM.sh` contain the settings and parameters used in our experiments. These scripts execute `reading.py` and can be modified and run as needed.

## Steering by ToM

To steer using different ToM components:

1. Train the related decoder model with the parameters and settings available in `scripts/train_steer.sh`.
2. Use the `control.py` file with the appropriate arguments to generate loss images and candidate files, which are stored in `./out`.

Refer to `scripts/table3_control.sh` for sample usage.

## Evaluation

For evaluation and generating results presented in the article, use the `table*.ipynb` notebooks:

- **CaSiNo and CraigslistBargain**: Accuracy is intuitively defined and explained in the article.
- **FanToM and Negotiation ToM**: Scores are adapted from the respective original repositories and papers.

Intermediate files generated during experiments are stored in output directories:
- Decoder model predictions are stored in `./controls`.
- Steered responses are saved in `./out`.

For FanToM's official code, some intermediate files must be moved to the expected location in `baselines/table2/CoT/FanToM/eval_fantom.py` and named according to its conventions.

## Repository Structure

The repository is organized as follows:

- `./baselines`: Contains baseline implementations.
- `./controls` and `./out`: Store intermediate files and results.
- `./scripts`: Contains sample scripts for running experiments.
- `./lit`: Copied from the LatentQA repository (refer to the original repo).
- `./data`: Contains datasets for experiments.

## Hardware Setting

Experiments were conducted on a virtual machine with 100 GB RAM and an H100_NVL GPU.
